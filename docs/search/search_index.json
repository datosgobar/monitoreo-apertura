{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Indice Monitoreo Apertura Monitoreo Apertura Versi\u00f3n : 0.0.31 Monitoreo Apertura es un desarrollo de la Administraci\u00f3n P\u00fablica Nacional que utiliza el framework Django para organizar una serie de m\u00f3dulos y funciones dbasado en CKAN . Es la tecnolog\u00eda detr\u00e1s del Portal Nacional de Datos Abiertos y de otros portales de datos abiertos de organismos de la APN, provincias y municipios. Comienzo r\u00e1pido Desarrolladores Instalaci\u00f3n Actualizaci\u00f3n Checklist para la puesta en producci\u00f3n Migraci\u00f3n Mantenimiento Configuraci\u00f3n HTTPS Configuraci\u00f3n de DNS Desarrollo Tests","title":"Inicio"},{"location":"#indice","text":"Monitoreo Apertura","title":"Indice"},{"location":"#monitoreo-apertura","text":"Versi\u00f3n : 0.0.31 Monitoreo Apertura es un desarrollo de la Administraci\u00f3n P\u00fablica Nacional que utiliza el framework Django para organizar una serie de m\u00f3dulos y funciones dbasado en CKAN . Es la tecnolog\u00eda detr\u00e1s del Portal Nacional de Datos Abiertos y de otros portales de datos abiertos de organismos de la APN, provincias y municipios. Comienzo r\u00e1pido Desarrolladores Instalaci\u00f3n Actualizaci\u00f3n Checklist para la puesta en producci\u00f3n Migraci\u00f3n Mantenimiento Configuraci\u00f3n HTTPS Configuraci\u00f3n de DNS Desarrollo Tests","title":"Monitoreo Apertura"},{"location":"usage/","text":"Indice Uso Carga de Nodos Configuraci\u00f3n de datasets indexables Generaci\u00f3n de archivos de configuraci\u00f3n Lectura de catalogos Cierre de la tarea Lectura periodica Generaci\u00f3n de indicadores Generaci\u00f3n peri\u00f3dica Reporte de indicadores Creaci\u00f3n de procesos Series de tiempo Importar/Exportar indicadores Uso Carga de Nodos Despues de iniciar sesion como Administrador, debemos cargar un nuevo Node Register file . Esta pagina se encuentra en la ruta /admin/django_datajsonar/noderegisterfile/ . Este archivo tiene un registro de los nodos a federar . Ese un archivo de extencion .yml y tiene un aspecto como el siguiente: datosgobar : url : \"http://datos.gob.ar/data.json\" formato : \"json\" federado : false transporte-bis : url : \"http://datos.transporte.gob.ar/data.json\" formato : \"json\" federado : false # Mas nodos... Luego de que creamos la nueva instancia, volvemos a la pagina del listado y deberiamos ver algo como la siguiente imagen: Luego seleccionamos la instancia y usamos la accion \"Process node file\", como se muestra en la imagen: Eso procesara el archivo (puede tardar un poco), y al terminar veremos los nodos detectados en /admin/django_datajsonar/node/ , algo parecido a Configuraci\u00f3n de datasets indexables Hay 2 formas de marcar un nodo como indexable, manualmente o cargando un csv de configuraci\u00f3n. Para el caso manual, se puede marcar en el modelo, o marcar un subconjunto de los datasets y ejecutar la acci\u00f3n \"Marcar como indexable\". El otro m\u00e9todo es cargando un nuevo Dataset Indexing file . Esta pagina se encuentra en la ruta /admin/django_datajsonar/datasetindexingfile/ . El archivo tiene un registro de los datasets indexables . Es un archivo de extensi\u00f3n .csv y tiene un aspecto como el siguiente: catalog_id,dataset_identifier sspm,399 sspm,330 enacom,REGIS-DE-PROVE-POSTA acumar,cb351aa5-731b-458b-8227-a0c5b828356f # M\u00e1s entradas La primera columna tiene el identificador del cat\u00e1logo, y la segunda el identificador del dataset que se desea marcar como indexable. Luego de que creamos la nueva instancia, volvemos a la p\u00e1gina del listado y deber\u00edamos ver algo como la siguiente imagen: Luego seleccionamos la instancia y usamos la acci\u00f3n \"Process node file\", como se muestra en la imagen: Eso procesa el archivo (puede tardar un poco), y al terminar veremos los datasets marcados como indexables en /admin/django_datajsonar/node/ . Generaci\u00f3n de archivos de configuraci\u00f3n Hay 2 formas de generar los archivos de configuraci\u00f3n con los datasets indexables. La primera es entrando en la ruta: admin/dataset/federacion-config.csv . De esa manera, se descarga un csv con todos datasets marcados como indexables. La segunda es mediante una acci\u00f3n de Django. Podemos seleccionar un subconjunto de datasets y ejecutar la acci\u00f3n Generar csv de configuraci\u00f3n De esta manera conseguimos el csv de configuraci\u00f3n con los datasets indexables del subconjunto elegido. Lectura de catalogos Para lanzar una lectura de todos los catalogos de los nodos, podemos instancia una ReadDataJsonTask . Para eso nos dirigimos a la ruta /admin/django_datajsonar/readdatajsontask/ . Esta instancia no requiere ningun parametro, ya que leera los datos necesarios de las instancias Node del proceso anterior. Esta instancia ira registrando los \"logs\" y \"resultados\" del proceso. Podremos ver algo como: Cierre de la tarea Por una cuestion de concurrencia, las tareas no quedaran en estado \"Finalizada\" por si solas. Para que el sistema verifique es estado de las tareas, debemos instanciar un RepeatableJob . Para eso vamos a la ruta /admin/scheduler/repeatablejob/ . En el campo nombre podemos poner lo que deseemos (como \"Cerrar lecturas de red\"). En el campo callable debemos poner django_datajsonar.indexing.tasks.close_read_datajson_task . En el campo Queue ponemos indexing . En los campos fecha y hora de scheduled time hacemos click en \"Hoy\" y \"Ahora\". Finalmente en interval ponemos 10 y en interval unit minutes . Luego de guardar la instancia deberiamos tener algo como: Lectura periodica Para que la lectura de los catalogos se ejecute periodicamente, debemos crear un RepeatableJob . Para eso vamos a la ruta /admin/scheduler/repeatablejob/ . En el campo nombre podemos poner lo que deseemos (como \"New Read Datajson Task\"). En el campo callable debemos poner django_datajsonar.tasks.schedule_new_read_datajson_task . En el campo Queue ponemos indexing . Habilitar el campo Enabled . En los campos fecha y hora de scheduled time hacemos click en \"Hoy\" y \"Ahora\". Finalmente en interval ponemos 1 y en interval unit days . Luego de guardar la instancia deberiamos tener algo como: Generaci\u00f3n de indicadores Hay 2 formas de comenzar una corrida de generaci\u00f3n de indicadores de la red de nodos: podemos instanciar una Corrida de indicadores. Para eso nos dirigimos a la ruta /admin/dashboard/indicatorsgenerationtask/ . Esta instancia no requiere ningun parametro, lee los cat\u00e1logos a partir de la librer\u00eda de Github. Estas instancias registran los \"logs\" y \"resultados\" del proceso. Podremos ver algo como: La otra forma es mediante un management command de Django. El comando python manage.py indicadores dispara de manera sincr\u00f3nica una tarea de generaci\u00f3n de indicadores. De la misma manera que el anterior, el resultado se guarda en los logs del IndicatorsGenerationTask correspondiente. Generaci\u00f3n peri\u00f3dica Para que la lectura de los catalogos se ejecute periodicamente, debemos crear un RepeatableJob . Para eso vamos a la ruta /admin/scheduler/repeatablejob/ . En el campo nombre podemos poner lo que deseemos (como \"Generaci\u00f3n indicadores\"). En el campo callable debemos poner monitoreo.apps.dashboard.indicators_tasks.indicators_run . En el campo Queue ponemos indicators . Habilitar el campo Enabled . En los campos fecha y hora de scheduled time hacemos click en \"Hoy\" y \"Ahora\". Finalmente en interval ponemos 1 y en interval unit days . Luego de guardar la instancia deberiamos tener algo como: Reporte de indicadores Es posible programar una tarea para enviar un reporte de los indicadores de la red de nodos a los responsables pertinentes. Se hace tambi\u00e9n con un RepeatableJob . En la ruta /admin/scheduler/repeatablejob/ . En el campo nombre podemos poner lo que deseemos (como \"Reporte indicadores\"). En el campo callable debemos poner monitoreo.apps.dashboard.report_tasks.send_reports . En el campo Queue ponemos reports . Habilitar el campo Enabled . En los campos fecha y hora de scheduled time hacemos click en \"Hoy\" y \"Ahora\". Finalmente en interval ponemos 1 y en interval unit days . Luego de guardar la instancia deberiamos tener algo como: Los mails de reporte de red se envian al staff del proyecto. Para marcar un usuario como staff, hay que acceder a la ruta /admin/auth/user/ y en la vista del usuario particular, marcar la opci\u00f3n: Creaci\u00f3n de procesos Es posible, crear procesos que engloben las tareas descriptas anteriormente y las ejecuten secuencialmente. Para lograr eso vamos a la ruta /admin/django_datajsonar/synchronizer/ . All\u00ed vamos a Create new process y nos encontramos con un formulario de este estilo: En el primer campo ponemos el nombre del proceso y en el segundo la cantidad de etapas que tendr\u00e1. Al dar submit, nos encontramos con la segunda parte del formulario: Cada fila representa una etapa, y se ejecutan en orden siendo la de arriba la primera. Actualmente para configurar las tareas ya existentes se deben pasar estos valores: Tarea Callable Str Queue Task Lectura de la red de nodos (default) django_datajsonar.tasks.schedule_new_read_datajson_task indexing ReadDataJsonTask Lectura de la red de nodos (completa) django_datajsonar.tasks.schedule_full_read_task indexing ReadDataJsonTask Lectura de la red de nodos (metadatos) django_datajsonar.tasks.schedule_metadata_read_task indexing ReadDataJsonTask Federaci\u00f3n de metadatos monitoreo.apps.dashboard.tasks.federation_run federation FederationTask C\u00e1lculo de indicadores monitoreo.apps.dashboard.indicators_tasks.indicators_run indicators IndicatorsGenerationTask Env\u00edo de reportes monitoreo.apps.dashboard.report_tasks.send_reports reports ReportGenerationTask Si no est\u00e1n creadas, es necesario schedulear 2 tareas de mantenimiento peri\u00f3dicas. Se crean mediante repeatableJobs: En la ruta /admin/scheduler/repeatablejob/ . Primero vamos a crear la tarea que comienza los procesos en stand-by. En el campo nombre podemos poner lo que deseemos (como \"Comenzar synchronizers\"). En el campo callable debemos poner django_datajsonar.synchronizer_tasks.start_synchros . En el campo Queue ponemos synchro . Habilitar el campo Enabled . En los campos fecha y hora de scheduled time hacemos click en \"Hoy\" y \"Ahora\". Finalmente en interval ponemos 1 y en interval unit days . Luego de guardar la instancia deberiamos tener algo como: La siguiente es la tarea que avanza las tareas a medida que van terminando: En el campo nombre podemos poner lo que deseemos (como \"Avanzar synchronizers\"). En el campo callable debemos poner django_datajsonar.synchronizer_tasks.upkeep . En el campo Queue ponemos synchro . Habilitar el campo Enabled . En los campos fecha y hora de scheduled time hacemos click en \"Hoy\" y \"Ahora\". Finalmente en interval ponemos 5 y en interval unit minutes . Luego de guardar la instancia deberiamos tener algo como: Series de tiempo Es posible descargar los indicadores como series de tiempo en formato .csv . Entrando en la ruta admin/dashboard/indicadorred/series-indicadores podemos descargar las series de los indicadores agregados; y en la ruta admin/dashboard/indicador/<id de nodo>/series-indicadores se encuentran las series para nodos particulares. Se pueden definir en el admin de tipos de indicadores cuales se desean mostrar en las series. Marcando los booleanos, series red y series nodos se especifican los tipos a presentar. Importar/Exportar indicadores Se pueden bajar la base de indicadores e indicadores de red mediante el management command: /manage.py export_indicators [file] [--aggregated] . El argumento file es un path al archivo donde se van a escribir los indicadores. El parametro opcional --aggregated indica si se trata de indicadores red, por default se asume indicadores de nodos. A su vez es posible importar a la base de indicadores mediante el management command /manage.py import_indicators [file] [--aggregated] Los par\u00e1metros file y --aggregated indican lo mismo que para el comando anterior. Se espera un csv v\u00e1lido con la forma: fecha, indicador_tipo, indicador_valor [, jurisdiccion_id, jurisdiccion_nombre] para indicadores de red y de nodos respectivamente. La operaci\u00f3n es un upsert, es decir se actualizaran los valores en caso que ya existen y se crearan indicadores nuevos si estos no est\u00e1n presentes.","title":"Usage"},{"location":"usage/#indice","text":"Uso Carga de Nodos Configuraci\u00f3n de datasets indexables Generaci\u00f3n de archivos de configuraci\u00f3n Lectura de catalogos Cierre de la tarea Lectura periodica Generaci\u00f3n de indicadores Generaci\u00f3n peri\u00f3dica Reporte de indicadores Creaci\u00f3n de procesos Series de tiempo Importar/Exportar indicadores","title":"Indice"},{"location":"usage/#uso","text":"","title":"Uso"},{"location":"usage/#carga-de-nodos","text":"Despues de iniciar sesion como Administrador, debemos cargar un nuevo Node Register file . Esta pagina se encuentra en la ruta /admin/django_datajsonar/noderegisterfile/ . Este archivo tiene un registro de los nodos a federar . Ese un archivo de extencion .yml y tiene un aspecto como el siguiente: datosgobar : url : \"http://datos.gob.ar/data.json\" formato : \"json\" federado : false transporte-bis : url : \"http://datos.transporte.gob.ar/data.json\" formato : \"json\" federado : false # Mas nodos... Luego de que creamos la nueva instancia, volvemos a la pagina del listado y deberiamos ver algo como la siguiente imagen: Luego seleccionamos la instancia y usamos la accion \"Process node file\", como se muestra en la imagen: Eso procesara el archivo (puede tardar un poco), y al terminar veremos los nodos detectados en /admin/django_datajsonar/node/ , algo parecido a","title":"Carga de Nodos"},{"location":"usage/#configuracion-de-datasets-indexables","text":"Hay 2 formas de marcar un nodo como indexable, manualmente o cargando un csv de configuraci\u00f3n. Para el caso manual, se puede marcar en el modelo, o marcar un subconjunto de los datasets y ejecutar la acci\u00f3n \"Marcar como indexable\". El otro m\u00e9todo es cargando un nuevo Dataset Indexing file . Esta pagina se encuentra en la ruta /admin/django_datajsonar/datasetindexingfile/ . El archivo tiene un registro de los datasets indexables . Es un archivo de extensi\u00f3n .csv y tiene un aspecto como el siguiente: catalog_id,dataset_identifier sspm,399 sspm,330 enacom,REGIS-DE-PROVE-POSTA acumar,cb351aa5-731b-458b-8227-a0c5b828356f # M\u00e1s entradas La primera columna tiene el identificador del cat\u00e1logo, y la segunda el identificador del dataset que se desea marcar como indexable. Luego de que creamos la nueva instancia, volvemos a la p\u00e1gina del listado y deber\u00edamos ver algo como la siguiente imagen: Luego seleccionamos la instancia y usamos la acci\u00f3n \"Process node file\", como se muestra en la imagen: Eso procesa el archivo (puede tardar un poco), y al terminar veremos los datasets marcados como indexables en /admin/django_datajsonar/node/ .","title":"Configuraci\u00f3n de datasets indexables"},{"location":"usage/#generacion-de-archivos-de-configuracion","text":"Hay 2 formas de generar los archivos de configuraci\u00f3n con los datasets indexables. La primera es entrando en la ruta: admin/dataset/federacion-config.csv . De esa manera, se descarga un csv con todos datasets marcados como indexables. La segunda es mediante una acci\u00f3n de Django. Podemos seleccionar un subconjunto de datasets y ejecutar la acci\u00f3n Generar csv de configuraci\u00f3n De esta manera conseguimos el csv de configuraci\u00f3n con los datasets indexables del subconjunto elegido.","title":"Generaci\u00f3n de archivos de configuraci\u00f3n"},{"location":"usage/#lectura-de-catalogos","text":"Para lanzar una lectura de todos los catalogos de los nodos, podemos instancia una ReadDataJsonTask . Para eso nos dirigimos a la ruta /admin/django_datajsonar/readdatajsontask/ . Esta instancia no requiere ningun parametro, ya que leera los datos necesarios de las instancias Node del proceso anterior. Esta instancia ira registrando los \"logs\" y \"resultados\" del proceso. Podremos ver algo como:","title":"Lectura de catalogos"},{"location":"usage/#cierre-de-la-tarea","text":"Por una cuestion de concurrencia, las tareas no quedaran en estado \"Finalizada\" por si solas. Para que el sistema verifique es estado de las tareas, debemos instanciar un RepeatableJob . Para eso vamos a la ruta /admin/scheduler/repeatablejob/ . En el campo nombre podemos poner lo que deseemos (como \"Cerrar lecturas de red\"). En el campo callable debemos poner django_datajsonar.indexing.tasks.close_read_datajson_task . En el campo Queue ponemos indexing . En los campos fecha y hora de scheduled time hacemos click en \"Hoy\" y \"Ahora\". Finalmente en interval ponemos 10 y en interval unit minutes . Luego de guardar la instancia deberiamos tener algo como:","title":"Cierre de la tarea"},{"location":"usage/#lectura-periodica","text":"Para que la lectura de los catalogos se ejecute periodicamente, debemos crear un RepeatableJob . Para eso vamos a la ruta /admin/scheduler/repeatablejob/ . En el campo nombre podemos poner lo que deseemos (como \"New Read Datajson Task\"). En el campo callable debemos poner django_datajsonar.tasks.schedule_new_read_datajson_task . En el campo Queue ponemos indexing . Habilitar el campo Enabled . En los campos fecha y hora de scheduled time hacemos click en \"Hoy\" y \"Ahora\". Finalmente en interval ponemos 1 y en interval unit days . Luego de guardar la instancia deberiamos tener algo como:","title":"Lectura periodica"},{"location":"usage/#generacion-de-indicadores","text":"Hay 2 formas de comenzar una corrida de generaci\u00f3n de indicadores de la red de nodos: podemos instanciar una Corrida de indicadores. Para eso nos dirigimos a la ruta /admin/dashboard/indicatorsgenerationtask/ . Esta instancia no requiere ningun parametro, lee los cat\u00e1logos a partir de la librer\u00eda de Github. Estas instancias registran los \"logs\" y \"resultados\" del proceso. Podremos ver algo como: La otra forma es mediante un management command de Django. El comando python manage.py indicadores dispara de manera sincr\u00f3nica una tarea de generaci\u00f3n de indicadores. De la misma manera que el anterior, el resultado se guarda en los logs del IndicatorsGenerationTask correspondiente.","title":"Generaci\u00f3n de indicadores"},{"location":"usage/#generacion-periodica","text":"Para que la lectura de los catalogos se ejecute periodicamente, debemos crear un RepeatableJob . Para eso vamos a la ruta /admin/scheduler/repeatablejob/ . En el campo nombre podemos poner lo que deseemos (como \"Generaci\u00f3n indicadores\"). En el campo callable debemos poner monitoreo.apps.dashboard.indicators_tasks.indicators_run . En el campo Queue ponemos indicators . Habilitar el campo Enabled . En los campos fecha y hora de scheduled time hacemos click en \"Hoy\" y \"Ahora\". Finalmente en interval ponemos 1 y en interval unit days . Luego de guardar la instancia deberiamos tener algo como:","title":"Generaci\u00f3n peri\u00f3dica"},{"location":"usage/#reporte-de-indicadores","text":"Es posible programar una tarea para enviar un reporte de los indicadores de la red de nodos a los responsables pertinentes. Se hace tambi\u00e9n con un RepeatableJob . En la ruta /admin/scheduler/repeatablejob/ . En el campo nombre podemos poner lo que deseemos (como \"Reporte indicadores\"). En el campo callable debemos poner monitoreo.apps.dashboard.report_tasks.send_reports . En el campo Queue ponemos reports . Habilitar el campo Enabled . En los campos fecha y hora de scheduled time hacemos click en \"Hoy\" y \"Ahora\". Finalmente en interval ponemos 1 y en interval unit days . Luego de guardar la instancia deberiamos tener algo como: Los mails de reporte de red se envian al staff del proyecto. Para marcar un usuario como staff, hay que acceder a la ruta /admin/auth/user/ y en la vista del usuario particular, marcar la opci\u00f3n:","title":"Reporte de indicadores"},{"location":"usage/#creacion-de-procesos","text":"Es posible, crear procesos que engloben las tareas descriptas anteriormente y las ejecuten secuencialmente. Para lograr eso vamos a la ruta /admin/django_datajsonar/synchronizer/ . All\u00ed vamos a Create new process y nos encontramos con un formulario de este estilo: En el primer campo ponemos el nombre del proceso y en el segundo la cantidad de etapas que tendr\u00e1. Al dar submit, nos encontramos con la segunda parte del formulario: Cada fila representa una etapa, y se ejecutan en orden siendo la de arriba la primera. Actualmente para configurar las tareas ya existentes se deben pasar estos valores: Tarea Callable Str Queue Task Lectura de la red de nodos (default) django_datajsonar.tasks.schedule_new_read_datajson_task indexing ReadDataJsonTask Lectura de la red de nodos (completa) django_datajsonar.tasks.schedule_full_read_task indexing ReadDataJsonTask Lectura de la red de nodos (metadatos) django_datajsonar.tasks.schedule_metadata_read_task indexing ReadDataJsonTask Federaci\u00f3n de metadatos monitoreo.apps.dashboard.tasks.federation_run federation FederationTask C\u00e1lculo de indicadores monitoreo.apps.dashboard.indicators_tasks.indicators_run indicators IndicatorsGenerationTask Env\u00edo de reportes monitoreo.apps.dashboard.report_tasks.send_reports reports ReportGenerationTask Si no est\u00e1n creadas, es necesario schedulear 2 tareas de mantenimiento peri\u00f3dicas. Se crean mediante repeatableJobs: En la ruta /admin/scheduler/repeatablejob/ . Primero vamos a crear la tarea que comienza los procesos en stand-by. En el campo nombre podemos poner lo que deseemos (como \"Comenzar synchronizers\"). En el campo callable debemos poner django_datajsonar.synchronizer_tasks.start_synchros . En el campo Queue ponemos synchro . Habilitar el campo Enabled . En los campos fecha y hora de scheduled time hacemos click en \"Hoy\" y \"Ahora\". Finalmente en interval ponemos 1 y en interval unit days . Luego de guardar la instancia deberiamos tener algo como: La siguiente es la tarea que avanza las tareas a medida que van terminando: En el campo nombre podemos poner lo que deseemos (como \"Avanzar synchronizers\"). En el campo callable debemos poner django_datajsonar.synchronizer_tasks.upkeep . En el campo Queue ponemos synchro . Habilitar el campo Enabled . En los campos fecha y hora de scheduled time hacemos click en \"Hoy\" y \"Ahora\". Finalmente en interval ponemos 5 y en interval unit minutes . Luego de guardar la instancia deberiamos tener algo como:","title":"Creaci\u00f3n de procesos"},{"location":"usage/#series-de-tiempo","text":"Es posible descargar los indicadores como series de tiempo en formato .csv . Entrando en la ruta admin/dashboard/indicadorred/series-indicadores podemos descargar las series de los indicadores agregados; y en la ruta admin/dashboard/indicador/<id de nodo>/series-indicadores se encuentran las series para nodos particulares. Se pueden definir en el admin de tipos de indicadores cuales se desean mostrar en las series. Marcando los booleanos, series red y series nodos se especifican los tipos a presentar.","title":"Series de tiempo"},{"location":"usage/#importarexportar-indicadores","text":"Se pueden bajar la base de indicadores e indicadores de red mediante el management command: /manage.py export_indicators [file] [--aggregated] . El argumento file es un path al archivo donde se van a escribir los indicadores. El parametro opcional --aggregated indica si se trata de indicadores red, por default se asume indicadores de nodos. A su vez es posible importar a la base de indicadores mediante el management command /manage.py import_indicators [file] [--aggregated] Los par\u00e1metros file y --aggregated indican lo mismo que para el comando anterior. Se espera un csv v\u00e1lido con la forma: fecha, indicador_tipo, indicador_valor [, jurisdiccion_id, jurisdiccion_nombre] para indicadores de red y de nodos respectivamente. La operaci\u00f3n es un upsert, es decir se actualizaran los valores en caso que ya existen y se crearan indicadores nuevos si estos no est\u00e1n presentes.","title":"Importar/Exportar indicadores"}]}